# 🚶 걸음걸이 감정 인식 시스템

![Python](https://img.shields.io/badge/Python-3.10-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-0.123.9-green)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![License](https://img.shields.io/badge/License-MIT-yellow)

**최고 정확도 96.99%를 달성한, 걸음걸이 패턴 기반의 실시간 감정 인식 AI 시스템**

사람의 걸음걸이(Gait)는 감정 상태에 따라 미묘하게 변화합니다. 이 프로젝트는 **KNN (K-Nearest Neighbors)** 머신러닝 모델을 활용하여 걸음걸이 데이터로부터 감정(행복, 슬픔, 분노 등)을 자동으로 예측하는 REST API를 제공합니다.

## 📖 목차

- [프로젝트 소개](#-프로젝트-소개)
- [주요 특징](#-주요-특징)
- [공익적 활용](#-공익적-활용)
- [기술 스택](#-기술-스택)
- [**성능 지표**](#-성능-지표)
- [**방법론: 14가지 수제 특징 (HCF)**](#-방법론-14가지-수제-특징-hand-crafted-features-hcf)
- [시작하기](#-시작하기)
  - [Docker로 실행 (권장)](#1-docker로-실행-권장)
  - [로컬 환경에서 실행](#2-로컬-환경에서-실행)
- [프로젝트 구조](#-프로젝트-구조)
- [API 사용법](#-api-사용법)
- [프론트엔드](#-프론트엔드)
- [테스트](#-테스트)
- [개발 가이드](#-개발-가이드)
- [문제 해결](#-문제-해결)
- [기여하기](#-기여하기)
- [라이선스](#-라이선스)

---

## 🎯 프로젝트 소개

걸음걸이 감정 인식(Gait-Emotion Recognition)은 사람의 독특한 걸음걸이 패턴을 분석하여 감정 상태를 추론하는 기술입니다. 이 프로젝트는 다음과 같은 과정으로 작동하며, **96.99%**의 높은 정확도를 달성했습니다. 

1. **데이터 입력**: MediaPipe 등에서 추출한 신체 키포인트(관절 좌표)
2. **특징 추출**: 걸음걸이의 **14가지 수제 특징(HCF)** 자동 추출 (보폭, 관절 변화율 등)
3. **감정 예측**: **KNN** 머신러닝 모델로 감정 분류 (최종 배포 모델)
4. **결과 반환**: 예측된 감정과 신뢰도를 JSON 형태로 반환

### 🔍 걸음걸이로 알 수 있는 것들

- **행복(Happy)**: 빠르고 경쾌한 걸음, 큰 보폭, 활발한 팔 움직임
- **슬픔(Sad)**: 느리고 무거운 걸음, 작은 보폭, 머리를 숙임
- **공포(Fear)**: 경계하는 걸음, 경직된 자세, 불규칙한 걸음
- **혐오(Disgust)**: 느린 속도, 움츠린 자세, 제한된 움직임
- **분노(Angry)**: 빠르지만 불규칙한 걸음, 경직된 자세
- **중립(Neutral)**: 평범하고 일정한 걸음걸이

---

## ✨ 주요 특징

- ✅ **RESTful API**: FastAPI 기반의 고성능 API
- ✅ **Docker 지원**: 어디서든 동일한 환경에서 실행 가능
- ✅ **최고 성능 달성**: **KNN 모델**로 **96.99%**의 최고 정확도 달성
- ✅ **실시간 최적화**: 추론 속도 **$0.048 \text{ ms}$**로 실시간 서비스에 완벽 적합
- ✅ **규칙 기반 Fallback**: 모델 파일 없이도 작동 가능
- ✅ **상세한 주석**: 비전공자도 이해할 수 있는 한글 주석
- ✅ **프론트엔드**: 웹 인터페이스로 쉽게 테스트 가능
- ✅ **자동 테스트**: pytest로 API 엔드포인트 검증
- ✅ **CORS 지원**: 프론트엔드에서 자유롭게 호출 가능

---

## 🌍 공익적 활용

이 기술은 다양한 공익적 목적으로 활용될 수 있습니다:

### 1. 범죄 예방 및 공공 안전
- **CCTV 분석**: 공공장소에서 이상 행동 감지
- **폭력 예방**: 분노나 공격성 징후를 조기 발견
- **실종자 수색**: 감정 상태로 위험 상황 파악

### 2. 군중 안전 경보 시스템
- **이벤트 모니터링**: 축제, 콘서트 등에서 군중의 감정 상태 파악
- **패닉 감지**: 대규모 인파에서 불안이나 공포 확산 감지
- **대피 유도**: 위험 상황 발생 시 신속한 대응

### 3. 놀이공원 감정 모니터링
- **고객 경험 분석**: 방문객의 만족도를 실시간으로 파악
- **안전 관리**: 불안이나 스트레스를 느끼는 방문객 조기 발견
- **서비스 개선**: 감정 데이터 기반 시설 및 서비스 최적화

### 4. 의료 및 헬스케어
- **우울증 모니터링**: 환자의 감정 상태 추적
- **노인 케어**: 요양원에서 노인의 정서 상태 파악
- **재활 치료**: 치료 진행 상황 객관적 평가

### 5. 기타 활용 분야
- **로봇 상호작용**: 로봇이 사람의 감정을 이해하고 적절히 반응
- **스마트 시티**: 도시 전체의 시민 행복도 측정
- **교육**: 학생들의 학습 스트레스 파악

---

## 🛠 기술 스택

### 백엔드
- **Python 3.10**: 주 프로그래밍 언어
- **FastAPI 0.123.9**: 고성능 웹 프레임워크
- **Uvicorn 0.38.0**: ASGI 서버
- **Pydantic 2.12.5**: 데이터 검증

### 머신러닝
- **scikit-learn 1.6.1**: **KNN** 분류기
- **NumPy 2.2.6**: 수치 연산
- **joblib 1.5.2**: 모델 저장/로드

### 컴퓨터 비전
- **OpenCV 4.12.0.88**: 이미지 및 비디오 처리 (HRNet/MediaPipe 연동 시)

### 인프라
- **Docker**: 컨테이너화
- **docker-compose**: 멀티 컨테이너 관리

### 개발 도구
- **pytest**: 테스트 프레임워크
- **httpx**: 비동기 HTTP 클라이언트

---

## 📊 성능 지표: 모델 비교 및 선정 결과 (Performance Benchmarks)

저희는 실시간 서비스 환경을 목표로 **최고 정확도 ($96.99\%$)**와 **최적의 추론 속도 ($0.048 \text{ ms}$)**를 모두 만족시키는 **KNN** 모델을 최종 배포 모델로 선정했습니다.

| 모델 아키텍처 | 사용 특징 (Features) | 정확도 (Accuracy) | 응답 시간 (Latency) | 비고 |
| :--- | :--- | :--- | :--- | :--- |
| **KNN (최종 배포)** | **14개 HCF** | **96.99%** | **0.048 ms** | **최고 성능 및 최적 속도 달성** |
| Bi-LSTM HCF Fusion | Raw Skeleton + 14개 HCF | 94.66% | 약 80ms | 최고 정확도 근접 딥러닝 모델 (실제 측정값 불가능) |
| Bi-LSTM | Raw Skeleton | 92.61% | 약 55ms | 시계열 딥러닝 비교군 |
| Random Forest | 14개 HCF | 72.81% | 0.072 ms | 빠른 응답 속도를 보인 ML 모델 |
| SVM | 14개 HCF | 34.42% | 약 15ms | 전통 ML 비교군 |

### ✅ 모델 선정 근거 (Deployment Rationale)

저희 프로젝트는 정확도와 추론 속도라는 두 가지 핵심 지표에서 가장 뛰어난 성능을 보인 **KNN** 모델을 최종 배포 모델로 선정했습니다.

* **최고 정확도 달성:** **KNN**은 모든 모델(딥러닝 포함) 중 가장 높은 **96.99%**의 압도적인 정확도를 기록했습니다. 이는 저희가 설계한 **14가지 수제 특징(HCF)**과 KNN의 조합이 걸음걸이 감정 인식에 **최적의 솔루션**임을 입증합니다.
* **실시간 처리 능력 확보:** KNN은 측정 결과 **0.048 ms**라는 극히 낮은 Latency를 보여줍니다. 이는 Random Forest($0.072 \text{ ms}$)보다도 빠르며, **실시간 웹캠 스트리밍 환경에서 지연 없는 사용자 경험**을 제공하는 핵심 근거가 됩니다.
* **결론:** 최고 정확도와 최적의 속도를 모두 만족시키는 **KNN**을 최종 FastAPI 배포 모델로 확정했습니다.

---

## 🧬 방법론: 14가지 수제 특징 (Hand-Crafted Features, HCF)

저희는 KNN 모델의 **96.99%** 정확도 달성에 핵심적인 역할을 한, MediaPipe 키포인트 데이터 기반의 14가지 독자적인 특징을 추출했습니다. 이 특징 추출 과정은 프로젝트의 핵심 기술이며, **`src/feature_extractor.py`**에 상세 로직이 구현되어 있습니다.

* **관절 각도 및 변화율:** 무릎, 엉덩이, 발목 관절의 평균/최대 각도 변화율
* **보폭 및 높이:** 평균 보폭 길이, 신체 중심점(Center of Mass)의 수직/수평 이동 범위
* **상체 자세:** 어깨와 골반 너비 비율, 상체 기울기 각도의 분산
* **속도 및 리듬:** 신체 부위별 이동 속도의 평균 및 분산, 걸음걸이 리듬 관련 특징 등

---

## 🚀 시작하기

### 환경 정보

이 프로젝트는 다음 환경에서 테스트되었습니다:

- **Python 버전**: 3.10
- **주요 패키지**:
  - FastAPI 0.123.9
  - **scikit-learn 1.6.1 (KNN)**
  - NumPy 2.2.6
  - OpenCV 4.12.0.88
- **지원 감정**: Happy, Sad, Fear, Disgust, Angry, Neutral (6가지)

### 사전 요구사항

#### Docker 사용 시 (권장)
- Docker 20.10 이상
- docker-compose 1.29 이상

#### 로컬 실행 시
- Python 3.10 이상
- pip

---

### 1. Docker로 실행 (권장)

가장 간단하고 빠른 방법입니다. 모든 의존성이 자동으로 설치됩니다.

```bash
# 1. 저장소 클론
git clone [https://github.com/KimTaek-Su/gait-emotion-recognition.git](https://github.com/KimTaek-Su/gait-emotion-recognition.git)
cd gait-emotion-recognition

# 2. Docker Compose로 실행
# Dockerfile과 main.py는 모델 로드 경로를 'models/deployment/KNN_best_model.joblib'로 가정합니다.
docker-compose up --build

# 서버가 http://localhost:8000 에서 실행됩니다